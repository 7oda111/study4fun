{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "detecting movements ",
      "provenance": [],
      "authorship_tag": "ABX9TyNEHjTo5voLI6VmMzrPQ6g2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7oda111/study4fun/blob/main/detecting_movements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import cv2\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf \n",
        "from PIL import Image\n",
        "from keras import models, layers, optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image as image_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "% matplotlib inline\n",
        "style.use('seaborn-whitegrid')\n",
        "warnings.filterwarnings(action='once')"
      ],
      "metadata": {
        "id": "Wcolp7UQMoTF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gestures = {'L_': 'L',\n",
        "           'fi': 'Fist',\n",
        "           'C_': 'C',\n",
        "           'ok': 'Okay',\n",
        "           'pe': 'Peace',\n",
        "           'pa': 'Palm'\n",
        "            }\n",
        "gestures_map = {'Fist' : 0,\n",
        "                'L': 1,\n",
        "                'Okay': 2,\n",
        "                'Palm': 3,\n",
        "                'Peace': 4\n",
        "                }"
      ],
      "metadata": {
        "id": "E-lq5CD7NMPC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(path):\n",
        "    img = Image.open(path)\n",
        "    img = img.resize((224, 224))\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "def process_data(X_data, y_data):\n",
        "    X_data = np.array(X_data, dtype = 'float32')\n",
        "    if rgb:\n",
        "        pass\n",
        "    else:\n",
        "        X_data = np.stack((X_data,)*3, axis=-1)\n",
        "    X_data /= 255\n",
        "    y_data = np.array(y_data)\n",
        "    y_data = to_categorical(y_data)\n",
        "    return X_data, y_data\n",
        "\n",
        "def walk_file_tree(relative_path):\n",
        "    X_data = []\n",
        "    y_data = [] \n",
        "    for directory, subdirectories, files in os.walk(relative_path):\n",
        "        for file in files:\n",
        "            if not file.startswith('.') and (not file.startswith('C_')):\n",
        "                path = os.path.join(directory, file)\n",
        "                gesture_name = gestures[file[0:2]]\n",
        "                y_data.append(gestures_map[gesture_name])\n",
        "                X_data.append(process_image(path))   \n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    X_data, y_data = process_data(X_data, y_data)\n",
        "    return X_data, y_data\n",
        "\n",
        "class Data(object):\n",
        "    def __init__(self):\n",
        "        self.X_data = []\n",
        "        self.y_data = []\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.X_data, self.y_data"
      ],
      "metadata": {
        "id": "trltrGJdOMqu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "relative_path = './frames/silhouettes/'\n",
        "rgb = False\n",
        "\n",
        "# # This method processes the data\n",
        "X_data, y_data = walk_file_tree(relative_path)\n",
        "\n",
        "# Can also optionally use a class to get this data, in order to keep it separate from Drawing data\n",
        "silhouette = Data()\n",
        "silhouette.X_data, silhouette.y_data = walk_file_tree(relative_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "bFL1-ZXZuYvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'X_data shape: {X_data.shape}')\n",
        "print(f'y_data shape: {y_data.shape}')"
      ],
      "metadata": {
        "id": "Slh1uLMduzv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.imshow(X_data[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "6LY5pMo0u2y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relative_path = './frames/drawings/'\n",
        "rgb = True\n",
        "\n",
        "# This method processes the data\n",
        "X_data, y_data = walk_file_tree(relative_path)"
      ],
      "metadata": {
        "id": "ftPbbc7Iu5jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(f'X_data shape: {X_data.shape}')\n",
        "print(f'y_data shape: {y_data.shape}')\n",
        "\n"
      ],
      "metadata": {
        "id": "k0r8Ixdpu-CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_data[0])"
      ],
      "metadata": {
        "id": "f2k7lSEpvA20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the dictionaries to a dataframe to be saved for future use\n",
        "# d = {'image_path':image_path, 'gesture':gesture, 'image_rgb': image_rgb, 'image_bw_x': X_data, 'image_bw_y': y_data}\n",
        "d = {'image_path':image_path, 'gesture':gesture}\n",
        "df = pd.DataFrame(d)\n",
        "# df['gesture_num'] = df['gesture'].apply(lambda x: x[1:2])\n",
        "# df['gesture_name'] = df['gesture'].apply(lambda x: x[3:])\n",
        "\n",
        "# df.to_csv('silhouette_df.csv')\n",
        "# df = pd.read_csv('silhouette_df.csv')"
      ],
      "metadata": {
        "id": "cQV6UsAyvDbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                2: 1,\n",
        "                7: 2,\n",
        "                1: 3,\n",
        "                'Peace': 4\n",
        "                }\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "root_dir = os.fsencode('./data/gestures_data/')\n",
        "\n",
        "for directory, subdirectories, files in os.walk(root_dir):\n",
        "    for file in files:\n",
        "        if not file.startswith(b'.'):\n",
        "            gesture_name = int(file.decode('utf8')[10:11])\n",
        "            if gesture_name in [1, 2, 3, 7]:\n",
        "                path = os.path.join(directory, file).decode('utf8')\n",
        "                y_data.append(gestures_map[gesture_name])\n",
        "\n",
        "                img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "                img = cv2.flip(img, 1)\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                blur = cv2.GaussianBlur(gray, (41, 41), 0)  #tuple indicates blur value\n",
        "                ret, thresh = cv2.threshold(blur, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "                thresh = cv2.resize(thresh, (224, 224))\n",
        "                thresh = np.array(thresh)\n",
        "                X_data.append(thresh)\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "process_data(X_data, y_data)"
      ],
      "metadata": {
        "id": "aBjXnhrvvHTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have to add a column of zeroes for the 'Peace' sign, since the Kaggle data does not have photos of \n",
        "# 'Peace' signs.\n",
        "z = np.zeros((len(y_data),1))\n",
        "y_data = np.append(y_data, z, axis=1)"
      ],
      "metadata": {
        "id": "wGr1sD9cvKRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_data.shape"
      ],
      "metadata": {
        "id": "o83H5lWevM3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.imshow(.5 - X_data[250])\n",
        "\n"
      ],
      "metadata": {
        "id": "AEqhAA0bvP6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.imshow(.5 - X_data[100])\n",
        "\n"
      ],
      "metadata": {
        "id": "lFmfXNp4vTAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(.5 - X_data[1200])"
      ],
      "metadata": {
        "id": "XJq6j7G5vWH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_rgb, X_test_rgb, y_train_rgb, y_test_rgb = train_test_split(image_rgb, y_data, test_size = 0.2, random_state=12, stratify=y_data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state=12, stratify=y_data)"
      ],
      "metadata": {
        "id": "x7HDX4glvYqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = './models/saved_model.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_acc',\n",
        "                               min_delta=0,\n",
        "                               patience=10,\n",
        "                               verbose=1,\n",
        "                               mode='auto',\n",
        "                               restore_best_weights=True)"
      ],
      "metadata": {
        "id": "qQXXOz7Uvbzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(imageSize, imageSize, 3))\n",
        "optimizer1 = optimizers.Adam()\n",
        "\n",
        "base_model = vgg_base  # Topless\n",
        "# Add top layer\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(128, activation='relu', name='fc3')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(64, activation='relu', name='fc4')(x)\n",
        "predictions = Dense(5, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Train top layers only\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_train, y_train), verbose=1,\n",
        "          callbacks=[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "rSQ57tbyveaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load VGG16\n",
        "# Get back the convolutional part of a VGG network trained on ImageNet\n",
        "\n",
        "imageSize = 224\n",
        "model1 = VGG16(weights='imagenet', include_top=False, input_shape=(imageSize, imageSize, 3))\n",
        "optimizer1 = optimizers.Adam()\n",
        "\n",
        "base_model = model1  # Topless\n",
        "# Add top layer\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(128, activation='relu', name='fc3')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(64, activation='relu', name='fc4')(x)\n",
        "\n",
        "predictions = Dense(5, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Train top layer\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_train, y_train), verbose=1,\n",
        "          callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "'''\n",
        "# Uncomment the section below and use in lieu of model.fit() above\n",
        "# if using image augmentation. Our final model did not.\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=45.,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(X_train)/32, epochs=150, validation_data=(X_test, y_test))\n",
        "'''"
      ],
      "metadata": {
        "id": "_BVHccCFvhZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# model.save('models/VGG_reversed.h5')\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('/home/ubuntu/project_kojak/models/VGG_reversed.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "6rNSvRE-vmQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classification_metrics(X_test, y_test):\n",
        "    pred = model.predict(X_test)\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    print(confusion_matrix(y_true, pred))\n",
        "    print('\\n')\n",
        "    print(classification_report(y_true, pred))"
      ],
      "metadata": {
        "id": "iCij1GzNvqF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gesture_names = {0: 'C',\n",
        "                 1: 'Fist',\n",
        "                 2: 'L',\n",
        "                 3: 'Okay',\n",
        "                 4: 'Palm',\n",
        "                 5: 'Peace'}\n",
        "\n",
        "def predict_rgb_image(path):\n",
        "    img2rgb = image_utils.load_img(path=path, target_size=(224, 224))\n",
        "    img2rgb = image_utils.img_to_array(img2rgb)\n",
        "    img2rgb = img2rgb.reshape(1, 224, 224, 3)\n",
        "    return gesture_names[np.argmax(model.predict(img2rgb))]"
      ],
      "metadata": {
        "id": "ElGNeiPivs5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "predict_rgb_image('images_to_predict/test - palm.jpg')\n",
        "\n"
      ],
      "metadata": {
        "id": "taGwTT_Cvxap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=45.,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen.fit(X_train_rgb)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(X_train_rgb, y_train_rgb, batch_size=32),\n",
        "                    steps_per_epoch=len(X_train_rgb) / 128, epochs=10, validation_data=(X_test_rgb, y_test_rgb))"
      ],
      "metadata": {
        "id": "K7q8hiQKv2Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "np.argmax(model.predict(X_test_rgb[0].reshape(1, 224, 224, 3)))\n",
        "\n"
      ],
      "metadata": {
        "id": "v5wFW4-9v20A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.25, seed=21))\n",
        "model.add(layers.Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "IiFNS5vwv5ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "cpm8pDXKv8Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train[:500], y_train[:500], epochs=4, batch_size=64, validation_data=(X_test[:500], y_test[:500]), verbose=1, callbacks = [MetricsCheckpoint('logs')])"
      ],
      "metadata": {
        "id": "5MI-Ad1Dv_Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_rgb, y_train_rgb, epochs=200, batch_size=16, validation_data=(X_test_rgb, y_test_rgb), verbose=1, callbacks =[early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "id": "6lLa3CQ9wGFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classification_reports(y_pred, y_true):\n",
        "    y_pred_classes = np.array(np.argmax(y_pred))  # reconverts back from one hot encoded \n",
        "    y_true = np.array(np.argmax(y_true))  # reconverts back from one hot encoded\n",
        "    print(confusion_matrix(y_true, y_pred_classes))\n",
        "    print(classification_report(y_true, y_pred_classes))"
      ],
      "metadata": {
        "id": "UjmpXhnkwJqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_pred_classes, y_true))"
      ],
      "metadata": {
        "id": "NbGaLFyCwMV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(confusion_matrix(y_pred_classes, y_true))\n",
        "\n"
      ],
      "metadata": {
        "id": "q-U4Lv5AwO5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}